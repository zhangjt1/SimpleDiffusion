{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf1wdgidirN_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Data\n",
        "\n",
        "Written by us"
      ],
      "metadata": {
        "id": "TgiZE2jUjTTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths of image directories\n",
        "train_imgs = '/content/drive/MyDrive/Colab Notebooks/EECS 442 final project/Resources/train'\n",
        "test_imgs = '/content/drive/MyDrive/Colab Notebooks/EECS 442 final project/Resources/test'\n",
        "train_annotations = '/content/drive/MyDrive/Colab Notebooks/EECS 442 final project/Resources/train_data_5000images.csv'\n",
        "test_annotations = '/content/drive/MyDrive/Colab Notebooks/EECS 442 final project/Resources/test_data.csv'"
      ],
      "metadata": {
        "id": "FD7Dz1X_i5Ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class HumanPoseDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        image = read_image(img_path)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "ZJVI-1l5i8jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "img_transform = transforms.Compose([transforms.CenterCrop(512),\n",
        "                                    transforms.Resize(64),\n",
        "                                    transforms.RandomHorizontalFlip(),\n",
        "                                    transforms.ToPILImage()\n",
        "                                    # transforms.ToTensor(), # Scales data into [0,1]\n",
        "                                    # transforms.Lambda(lambda t: (t * 2) - 1) # Scale between [-1, 1]\n",
        "                                    ])\n",
        "\n",
        "train_pose_dataset = HumanPoseDataset(annotations_file=train_annotations, img_dir=train_imgs, transform=img_transform)\n",
        "test_pose_dataset = HumanPoseDataset(annotations_file=test_annotations, img_dir=test_imgs, transform=img_transform)"
      ],
      "metadata": {
        "id": "tb4e67VCjAE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_images(datset, num_samples=20, cols=4):\n",
        "    \"\"\" Plots some samples from the dataset \"\"\"\n",
        "    plt.figure(figsize=(15,15))\n",
        "    for i, img in enumerate(data):\n",
        "        if i == num_samples:\n",
        "            break\n",
        "        plt.subplot(int(num_samples/cols) + 1, cols, i + 1)\n",
        "        plt.imshow(img[0])\n",
        "\n",
        "data = torchvision.datasets.Flowers102(root=\".\", download=True)\n",
        "show_images(data)"
      ],
      "metadata": {
        "id": "YVrbvX-ajBcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward Diffusion\n",
        "\n",
        "Reused"
      ],
      "metadata": {
        "id": "2AC15N6IjWh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
        "    return torch.linspace(start, end, timesteps)\n",
        "\n",
        "def get_index_from_list(vals, t, x_shape):\n",
        "    \"\"\"\n",
        "    Returns a specific index t of a passed list of values vals\n",
        "    while considering the batch dimension.\n",
        "    \"\"\"\n",
        "    batch_size = t.shape[0]\n",
        "    out = vals.gather(-1, t.cpu())\n",
        "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
        "\n",
        "def forward_diffusion_sample(x_0, t, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Takes an image and a timestep as input and\n",
        "    returns the noisy version of it\n",
        "    \"\"\"\n",
        "    noise = torch.randn_like(x_0)\n",
        "    sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x_0.shape\n",
        "    )\n",
        "\n",
        "    # mean + variance\n",
        "    return sqrt_alphas_cumprod_t.to(device) * x_0.to(device) \\\n",
        "    + sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)\n",
        "\n",
        "\n",
        "# Define beta schedule\n",
        "T = 500\n",
        "betas = linear_beta_schedule(timesteps=T)\n",
        "\n",
        "# Pre-calculate different terms for closed form\n",
        "alphas = 1. - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
        "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)"
      ],
      "metadata": {
        "id": "kjP790tejEsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "IMG_SIZE = 64\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "\n",
        "def load_transformed_dataset():\n",
        "    data_transforms = [\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(), # Scales data into [0,1]\n",
        "        transforms.Lambda(lambda t: (t * 2) - 1) # Scale between [-1, 1]\n",
        "    ]\n",
        "    data_transform = transforms.Compose(data_transforms)\n",
        "\n",
        "    train = torchvision.datasets.Flowers102(root=\".\", download=True,\n",
        "                                         transform=data_transform)\n",
        "\n",
        "    test = torchvision.datasets.Flowers102(root=\".\", download=True,\n",
        "                                         transform=data_transform, split='test')\n",
        "    return torch.utils.data.ConcatDataset([train, test])\n",
        "def show_tensor_image(image):\n",
        "    reverse_transforms = transforms.Compose([\n",
        "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
        "        transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
        "        transforms.Lambda(lambda t: t * 255.),\n",
        "        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
        "        transforms.ToPILImage(),\n",
        "    ])\n",
        "\n",
        "    # Take first image of batch\n",
        "    if len(image.shape) == 4:\n",
        "        image = image[0, :, :, :]\n",
        "    plt.imshow(reverse_transforms(image))\n",
        "\n",
        "data = load_transformed_dataset()\n",
        "dataloader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "print(len(dataloader.dataset))"
      ],
      "metadata": {
        "id": "fLkr_P4DjIpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backward Diffusion Models\n",
        "V1 is pieced together from many sources\n",
        "\n",
        "V2 is written"
      ],
      "metadata": {
        "id": "5A1TIx1FjY6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import math\n",
        "\n",
        "# SIMPLE DIFFUSION V1\n",
        "class Up(nn.Module):\n",
        "  def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
        "        super().__init__()\n",
        "        self.time_mlp =  nn.Linear(time_emb_dim, out_ch)\n",
        "        self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
        "        self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
        "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
        "        self.relu  = nn.ReLU()\n",
        "\n",
        "  def forward(self, x, t, ):\n",
        "        # First Conv\n",
        "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
        "        # Time embedding\n",
        "        time_emb = self.relu(self.time_mlp(t))\n",
        "        # Extend last 2 dimensions\n",
        "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
        "        # Add time channel\n",
        "        h = h + time_emb\n",
        "        # Second Conv\n",
        "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
        "        # Down or Upsample\n",
        "        return self.transform(h)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_emb_dim):\n",
        "        super().__init__()\n",
        "        self.time_mlp =  nn.Linear(time_emb_dim, out_ch)\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
        "        self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
        "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
        "        self.relu  = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, t, ):\n",
        "        # First Conv\n",
        "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
        "        # Time embedding\n",
        "        time_emb = self.relu(self.time_mlp(t))\n",
        "        # Extend last 2 dimensions\n",
        "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
        "        # Add time channel\n",
        "        h = h + time_emb\n",
        "        # Second Conv\n",
        "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
        "        # Down or Upsample\n",
        "        return self.transform(h)\n",
        "\n",
        "\n",
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        position = torch.arange(0, half_dim, device=device, dtype=torch.float32)\n",
        "        angle_rates = torch.exp(torch.arange(0, self.dim, 2, device=device).float() * -(math.log(10000.0) / self.dim))\n",
        "        angles = time.unsqueeze(-1) * angle_rates\n",
        "\n",
        "        # Use sine for the first half of the embeddings and cosine for the second half\n",
        "        embeddings = torch.cat([angles.sin(), angles.cos()], dim=-1)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class SimpleUnet(nn.Module):\n",
        "    \"\"\"\n",
        "    A simplified variant of the Unet architecture.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        image_channels = 3\n",
        "        down_channels = (64, 128, 256, 512, 1024)\n",
        "        up_channels = (1024, 512, 256, 128, 64)\n",
        "        out_dim = 3\n",
        "        time_emb_dim = 32\n",
        "\n",
        "        # Time embedding\n",
        "        self.time_mlp = nn.Sequential(\n",
        "                SinusoidalPositionEmbeddings(time_emb_dim),\n",
        "                nn.Linear(time_emb_dim, time_emb_dim),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "\n",
        "        # Initial projection\n",
        "        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)\n",
        "\n",
        "        # Downsample\n",
        "        self.downs = nn.ModuleList([Down(down_channels[i], down_channels[i+1], \\\n",
        "                                    time_emb_dim) \\\n",
        "                    for i in range(len(down_channels)-1)])\n",
        "        # Upsample\n",
        "        self.ups = nn.ModuleList([Up(up_channels[i], up_channels[i+1], \\\n",
        "                                        time_emb_dim) \\\n",
        "                    for i in range(len(up_channels)-1)])\n",
        "\n",
        "        self.output = nn.Conv2d(up_channels[-1], out_dim, 1)\n",
        "\n",
        "    def forward(self, x, timestep):\n",
        "        # Embedd time\n",
        "        t = self.time_mlp(timestep)\n",
        "        # Initial conv\n",
        "        x = self.conv0(x)\n",
        "        # Unet\n",
        "        residual_inputs = []\n",
        "        for down in self.downs:\n",
        "            x = down(x, t)\n",
        "            residual_inputs.append(x)\n",
        "        for up in self.ups:\n",
        "            residual_x = residual_inputs.pop()\n",
        "            # Add residual x as additional channels\n",
        "            x = torch.cat((x, residual_x), dim=1)\n",
        "            x = up(x, t)\n",
        "        return self.output(x)\n",
        "\n",
        "model = SimpleUnet()\n",
        "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
        "model"
      ],
      "metadata": {
        "id": "InKw6KXtjblP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SIMPLE DIFFUSION V2.1\n",
        "from torch import nn\n",
        "import math\n",
        "\n",
        "a = torch.tensor([2])\n",
        "r = torch.rsqrt(a).cuda() # 1/sqrt(2)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
        "        super(Block, self).__init__()\n",
        "\n",
        "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
        "\n",
        "        if up:\n",
        "            self.conv1 = nn.Conv2d(2 * in_ch, out_ch, kernel_size=3, padding=1)\n",
        "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, kernel_size=4, stride=2, padding=1)\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1)\n",
        "            self.transform = nn.Conv2d(out_ch, out_ch, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1)\n",
        "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
        "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        # First Conv\n",
        "        h = self.conv1(x)\n",
        "        h = self.bnorm1(h)\n",
        "        h = self.leaky_relu(h)\n",
        "\n",
        "        # Time embedding\n",
        "        time_emb = self.time_mlp(t)\n",
        "        time_emb = time_emb.unsqueeze(-1).unsqueeze(-1)\n",
        "        h = h + time_emb\n",
        "\n",
        "        # Second Conv\n",
        "        h = self.conv2(h)\n",
        "        h = self.bnorm2(h)\n",
        "        h = self.leaky_relu(h)\n",
        "\n",
        "        # Down or Upsample\n",
        "        return self.transform(h)\n",
        "\n",
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        position = torch.arange(0, half_dim, device=device, dtype=torch.float32)\n",
        "        angle_rates = torch.exp(torch.arange(0, self.dim, 2, device=device).float() * -(math.log(10000.0) / self.dim))\n",
        "        angles = time.unsqueeze(-1) * angle_rates\n",
        "\n",
        "        # Use sine for the first half of the embeddings and cosine for the second half\n",
        "        embeddings = torch.cat([angles.sin(), angles.cos()], dim=-1)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "class SimpleUnet(nn.Module):\n",
        "    \"\"\"\n",
        "    A simplified variant of the Unet architecture.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        image_channels = 3\n",
        "        down_channels = (64, 128, 256, 512, 1024)\n",
        "        up_channels = (1024, 512, 256, 128, 64,)\n",
        "        out_dim = 3\n",
        "        time_emb_dim = 32\n",
        "\n",
        "        # Time embedding\n",
        "        self.time_mlp = nn.Sequential(\n",
        "                SinusoidalPositionEmbeddings(time_emb_dim),\n",
        "                nn.Linear(time_emb_dim, time_emb_dim),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "\n",
        "        # Initial projection\n",
        "        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)\n",
        "\n",
        "        # Downsample\n",
        "        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i+1], \\\n",
        "                                    time_emb_dim) \\\n",
        "                    for i in range(len(down_channels)-1)])\n",
        "        # Upsample\n",
        "        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i+1], \\\n",
        "                                        time_emb_dim, up=True) \\\n",
        "                    for i in range(len(up_channels)-1)])\n",
        "\n",
        "        self.output = nn.Conv2d(up_channels[-1], out_dim, 1)\n",
        "\n",
        "    def forward(self, x, timestep):\n",
        "        # Embedd time\n",
        "        t = self.time_mlp(timestep)\n",
        "        # Initial conv\n",
        "        x = self.conv0(x)\n",
        "        # Unet\n",
        "        residual_inputs = []\n",
        "        for down in self.downs:\n",
        "            x = down(x, t)\n",
        "            residual_inputs.append(x)\n",
        "        for up in self.ups:\n",
        "            residual_x = residual_inputs.pop() * r\n",
        "            # Add residual x as additional channels\n",
        "            x = torch.cat((x, residual_x), dim=1)\n",
        "            x = up(x, t)\n",
        "        return self.output(x)\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('leaky_relu', 0.2))\n",
        "\n",
        "model = SimpleUnet()\n",
        "model.apply(weights_init)\n",
        "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
        "model"
      ],
      "metadata": {
        "id": "iOQSvJK9jPVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SIMPLE DIFFUSION V3, NOT WORKING\n",
        "from torch import nn\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.scale_factor = math.sqrt(d_model)\n",
        "\n",
        "    def forward(self, Q, K, V):\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale_factor\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        return output\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False, attn_dim=32):\n",
        "        super(Block, self).__init__()\n",
        "\n",
        "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
        "\n",
        "        if up:\n",
        "            self.conv1 = nn.Conv2d(2 * in_ch, out_ch, kernel_size=3, padding=1)\n",
        "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, kernel_size=4, stride=2, padding=1)\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1)\n",
        "            self.transform = nn.Conv2d(out_ch, out_ch, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        # Remove BatchNorm for increased speed (can be added back if needed)\n",
        "        # self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
        "\n",
        "        self.attention = SelfAttention(out_ch)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1)\n",
        "        # Remove BatchNorm for increased speed (can be added back if needed)\n",
        "        # self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        # First Conv\n",
        "        h = self.conv1(x)\n",
        "        # Remove BatchNorm for increased speed (can be added back if needed)\n",
        "        # h = self.bnorm1(h)\n",
        "        h = self.relu(h)\n",
        "\n",
        "        # Time embedding\n",
        "        time_emb = self.time_mlp(t)\n",
        "        time_emb = time_emb.unsqueeze(-1).unsqueeze(-1)\n",
        "        h = h + time_emb\n",
        "\n",
        "        # Scaled Dot-Product Attention\n",
        "        Q = self.attention(h, h, h)\n",
        "        h = h + Q  # Residual connection\n",
        "\n",
        "        # Second Conv\n",
        "        h = self.conv2(h)\n",
        "        # Remove BatchNorm for increased speed (can be added back if needed)\n",
        "        # h = self.bnorm2(h)\n",
        "        h = self.relu(h)\n",
        "\n",
        "        # Down or Upsample\n",
        "        return self.transform(h)\n",
        "\n",
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        position = torch.arange(0, half_dim, device=device, dtype=torch.float32)\n",
        "        angle_rates = torch.exp(torch.arange(0, self.dim, 2, device=device).float() * -(math.log(10000.0) / self.dim))\n",
        "        angles = time.unsqueeze(-1) * angle_rates\n",
        "\n",
        "        # Use sine for the first half of the embeddings and cosine for the second half\n",
        "        embeddings = torch.cat([angles.sin(), angles.cos()], dim=-1)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "class SimpleUnet(nn.Module):\n",
        "    \"\"\"\n",
        "    A simplified variant of the Unet architecture.\n",
        "    \"\"\"\n",
        "    def weights_init(self, m):\n",
        "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        image_channels = 3\n",
        "        down_channels = (64, 128, 256, 512, 1024)\n",
        "        up_channels = (1024, 512, 256, 128, 64)\n",
        "        out_dim = 3\n",
        "        time_emb_dim = 32\n",
        "\n",
        "        # Time embedding\n",
        "        self.time_mlp = nn.Sequential(\n",
        "                SinusoidalPositionEmbeddings(time_emb_dim),\n",
        "                nn.Linear(time_emb_dim, time_emb_dim),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "\n",
        "        # Initial projection\n",
        "        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)\n",
        "\n",
        "        # Downsample\n",
        "        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i+1], time_emb_dim, attn_dim=32)\n",
        "                                    for i in range(len(down_channels)-1)])\n",
        "        # Upsample\n",
        "        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i+1], time_emb_dim, up=True, attn_dim=32)\n",
        "                                  for i in range(len(up_channels)-1)])\n",
        "\n",
        "        # Edit: Corrected a bug found by Jakub C (see YouTube comment)\n",
        "        self.output = nn.Conv2d(up_channels[-1], out_dim, 1)\n",
        "\n",
        "        self.apply(self.weights_init)\n",
        "\n",
        "    def forward(self, x, timestep):\n",
        "        # Embedd time\n",
        "        t = self.time_mlp(timestep)\n",
        "        # Initial conv\n",
        "        x = self.conv0(x)\n",
        "        # Unet\n",
        "        residual_inputs = []\n",
        "        for down in self.downs:\n",
        "            x = down(x, t)\n",
        "            residual_inputs.append(x)\n",
        "        for up in self.ups:\n",
        "            residual_x = residual_inputs.pop()\n",
        "            # Add residual x as additional channels\n",
        "            x = torch.cat((x, residual_x), dim=1)\n",
        "            x = up(x, t)\n",
        "        return self.output(x)\n",
        "\n",
        "model = SimpleUnet()\n",
        "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
        "model\n"
      ],
      "metadata": {
        "id": "zzHD3Rgwapbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss\n",
        "Reused"
      ],
      "metadata": {
        "id": "g4F9JlaejqA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss(model, x_0, t):\n",
        "    x_noisy, noise = forward_diffusion_sample(x_0, t, device)\n",
        "    noise_pred = model(x_noisy, t)\n",
        "    return F.l1_loss(noise, noise_pred)"
      ],
      "metadata": {
        "id": "lQGUp8kXjpWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def sample_timestep(x, t):\n",
        "    \"\"\"\n",
        "    Calls the model to predict the noise in the image and returns\n",
        "    the denoised image.\n",
        "    Applies noise to this image, if we are not in the last step yet.\n",
        "    \"\"\"\n",
        "    betas_t = get_index_from_list(betas, t, x.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
        "    )\n",
        "    sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape)\n",
        "\n",
        "    # Call model (current image - noise prediction)\n",
        "    model_mean = sqrt_recip_alphas_t * (\n",
        "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
        "    )\n",
        "    posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape)\n",
        "\n",
        "    if t == 0:\n",
        "        # The t's are offset from the t's in the paper\n",
        "        return model_mean\n",
        "    else:\n",
        "        noise = torch.randn_like(x)\n",
        "        return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_plot_image():\n",
        "    # Sample noise\n",
        "    img_size = IMG_SIZE\n",
        "    img = torch.randn((1, 3, img_size, img_size), device=device)\n",
        "    plt.figure(figsize=(15,15))\n",
        "    plt.axis('off')\n",
        "    num_images = 10\n",
        "    stepsize = int(T/num_images)\n",
        "\n",
        "    for i in range(0,T)[::-1]:\n",
        "        t = torch.full((1,), i, device=device, dtype=torch.long)\n",
        "        img = sample_timestep(img, t)\n",
        "        # Edit: This is to maintain the natural range of the distribution\n",
        "        img = torch.clamp(img, -1.0, 1.0)\n",
        "        if i % stepsize == 0:\n",
        "            plt.subplot(1, num_images, int(i/stepsize)+1)\n",
        "            show_tensor_image(img.detach().cpu())\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QG0X0UJujw0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Patch Diffusion"
      ],
      "metadata": {
        "id": "416lTLrQmN1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For sample\n",
        "def sample_patchify(image, device=\"cuda\"):\n",
        "  image = torch.squeeze(image)\n",
        "  h, w = image.size(1), image.size(2)\n",
        "  rows = torch.arange(h, dtype=torch.long, device=device).repeat(h, 1)\n",
        "  columns = torch.arange(w, dtype=torch.long, device=device).view(-1, 1).repeat(1, w)\n",
        "  image_pos = torch.stack((rows, columns))\n",
        "\n",
        "  return torch.concat((image, image_pos), ).unsqueeze(0)\n",
        "\n",
        "\n",
        "# Patch Diffusion\n",
        "def patchify(images, patch_size=32, device=\"cuda\"):\n",
        "  batch_size, resolution = images.size(0), images.size(2)\n",
        "  h, w = images.size(2), images.size(3)\n",
        "  th, tw = patch_size, patch_size\n",
        "  # Randomly sample patch upper-left corner pixel\n",
        "  if w == tw and h == th:\n",
        "      i = torch.zeros((batch_size,), device=device).long()\n",
        "      j = torch.zeros((batch_size,), device=device).long()\n",
        "  else:\n",
        "      i = torch.randint(0, h - th + 1, (batch_size,), device=device)\n",
        "      j = torch.randint(0, w - tw + 1, (batch_size,), device=device)\n",
        "\n",
        "  # Make a tensor of the indexes of the patch\n",
        "  rows = torch.arange(th, dtype=torch.long, device=device) + i[:, None]\n",
        "  columns = torch.arange(tw, dtype=torch.long, device=device) + j[:, None]\n",
        "  images = images.to(device)\n",
        "  images = images.permute(1, 0, 2, 3)\n",
        "  images = images[:, torch.arange(batch_size)[:, None, None], rows[:, torch.arange(th)[:, None]],\n",
        "            columns[:, None]]\n",
        "  images = images.permute(1, 0, 2, 3)\n",
        "\n",
        "  # Repeat along batch size\n",
        "  x_pos = torch.arange(tw, dtype=torch.long, device=device).unsqueeze(0).repeat(th, 1).unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1, 1)\n",
        "  y_pos = torch.arange(th, dtype=torch.long, device=device).unsqueeze(1).repeat(1, tw).unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1, 1)\n",
        "\n",
        "  # Normalize to -1 to 1\n",
        "  x_pos = x_pos + j.view(-1, 1, 1, 1)\n",
        "  y_pos = y_pos + i.view(-1, 1, 1, 1)\n",
        "  x_pos = (x_pos / (resolution - 1) - 0.5) * 2.\n",
        "  y_pos = (y_pos / (resolution - 1) - 0.5) * 2.\n",
        "\n",
        "  images_pos = torch.cat((x_pos, y_pos), dim=1)\n",
        "\n",
        "  return images, images_pos\n",
        "\n",
        "\n",
        "def patch_loss(model, batch, t, patch_size, resolution, device=\"cuda\"):\n",
        "  images, images_pos = patchify(batch, patch_size)\n",
        "  # print(images.size())\n",
        "  # print(images_pos.size())\n",
        "  # loss = model(images, t, images_pos)\n",
        "  loss = model.forward(images, t)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "aZwxheRYmNiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "We wrote this code"
      ],
      "metadata": {
        "id": "AJ9XmYjnjsnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "from statistics import mean\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "import pickle\n",
        "import time\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "optimizer = Adam(model.parameters(), lr=0.00125)\n",
        "epochs = 100 # Try more!\n",
        "\n",
        "bestLoss = None\n",
        "\n",
        "lossOverTime = []\n",
        "\n",
        "# LOAD_FLOWER_PATH = \"/content/drive/MyDrive/EECS 442 final project/Resources/simple_diffusion_models/flowers2.pth\"\n",
        "SAVE_FLOWER_PATH = \"/content/drive/MyDrive/EECS 442 final project/Resources/simple_diffusion_models/flowers3.pth\"\n",
        "SAVE_FLOWER_PATH_COLLAB_FOLDER = \"//content/drive/MyDrive/Colab Notebooks/EECS 442 final project/Resources/simple_diffusion_models/flowers3.pth\"\n",
        "\n",
        "# model.load_state_dict(torch.load(FLOWER_PATH))\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    losses = []\n",
        "    print(\"Epoch\", epoch)\n",
        "    for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      t = torch.randint(0, T, (BATCH_SIZE,), device=device).long()\n",
        "      loss = get_loss(model, batch[0], t)\n",
        "      losses.append(loss.item())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if epoch % 5 == 0 and step == 0:\n",
        "        # Open a file and use dump()\n",
        "        # with open('/content/drive/MyDrive/EECS 442 final project/Resources/simple_diffusion_models/flower_losses.pkl', 'wb') as file:\n",
        "        with open('/content/drive/MyDrive/Colab Notebooks/EECS 442 final project/Resources/simple_diffusion_models/flower_losses.pkl', 'wb') as file:\n",
        "            # A new file will be created\n",
        "            pickle.dump(lossOverTime, file)\n",
        "        print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()} \")\n",
        "        sample_plot_image()\n",
        "\n",
        "    meanLoss = mean(losses)\n",
        "    lossOverTime.append(meanLoss)\n",
        "    print(\"Mean Loss:\", meanLoss)\n",
        "    if not bestLoss:\n",
        "      bestLoss = meanLoss - 0.002\n",
        "      continue\n",
        "    if meanLoss < bestLoss:\n",
        "      print(\"New Best Loss:\", meanLoss)\n",
        "      bestLoss = meanLoss\n",
        "      torch.save(model.state_dict(), SAVE_FLOWER_PATH_COLLAB_FOLDER)\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(f\"Total training time: {training_time} seconds\")"
      ],
      "metadata": {
        "id": "BmKWfHxWjsWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize results\n",
        "model.load_state_dict(torch.load(SAVE_FLOWER_PATH_COLLAB_FOLDER))\n",
        "sample_plot_image()"
      ],
      "metadata": {
        "id": "S21YYo9Fj13v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot losses\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the pickle file\n",
        "pickle_file_path = '/content/drive/MyDrive/Colab Notebooks/EECS 442 final project/Resources/simple_diffusion_models/flower_losses.pkl'\n",
        "with open(pickle_file_path, 'rb') as file:\n",
        "    loss_over_time = pickle.load(file)\n",
        "\n",
        "# Plot the loss over time\n",
        "plt.plot(loss_over_time, label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XbTUtg0tj5gi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}